{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTC-USD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to assign labels to the data for our model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    \"\"\"Function takes in the current and future price of a crypto-currency or stock.\n",
    "    Returns 1 of price increased in the future and 0 if price decrease.\"\"\"\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function takes a Dataframe as input processes data and return the feature (`X`) and target (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \"\"\"Function takes in a \"\"\"\n",
    "    \n",
    "    # Droppig label column\n",
    "    df = df.drop(\"future\", axis=1)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != \"target\": # Normalize all columns except target (label)\n",
    "            # Percent change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            df[col] = df[col].pct_change()\n",
    "            # Removing NaN values created by pct_change\n",
    "            df.dropna(inplace=True)\n",
    "            # Scale values to be between 0-1\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    \n",
    "    df.dropna(inplace=True) # Dropping NaN just in case!\n",
    "    sequential_data = [] # List to hold the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN) # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:\n",
    "        # Store all columns except target\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            # Append feature (a sequence of prices) and target (label 0 = Fall, 1 = Rise)\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "    \n",
    "    # Shuffling data\n",
    "    random.shuffle(sequential_data)\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    # Shuffling data\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys), len(sells))\n",
    "\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "\n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)     # X is the sequence of prices (feature data)\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "    return np.array(X), y\n",
    "#######################################FUNCTION END#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in data from several files in a directory and joining them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 | BCH-USD.csv | Processing...\n",
      "File 2 | BTC-USD.csv | Processing...\n",
      "File 3 | ETH-USD.csv | Processing...\n",
      "File 4 | LTC-USD.csv | Processing...\n",
      "Process complete!\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"Data//crypto\"\n",
    "\n",
    "# Empty DataFrame\n",
    "main_df = pd.DataFrame()\n",
    "for i, file in enumerate(os.listdir(DIRECTORY)):\n",
    "    ratio = file.split(\".\")[0]\n",
    "    print(\"File\", i+1, \"|\", file, \"| Processing...\")\n",
    "    dataset = os.path.join(DIRECTORY, file)\n",
    "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "    \n",
    "    # Rename volume and close to include the ticker so we can still which close/volume is which:\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "    \n",
    "    # Set time as index so we can join them on this shared time\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    # Ignore the other columns besides price and volume\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    if len(main_df)==0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward filling any missing data\n",
    "main_df.fillna(method=\"ffill\", inplace=True)\n",
    "main_df.dropna(inplace=True)\n",
    "\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "\n",
    "main_df.dropna(inplace=True)\n",
    "\n",
    "# Get the timestamps\n",
    "times = sorted(main_df.index.values)\n",
    "# Get the last 5% of the times\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]\n",
    "\n",
    "# Make the validation data where the index is in the last 5%\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "# Now the main_df is all the data up to the last 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 75002 validation: 3812\n",
      "Dont buys: 37501, buys: 37501\n",
      "VALIDATION Dont buys: 1906, buys: 1906\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Hidden layer 1\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Hidden layer 2\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Hidden layer 3\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Tensorboard callback\n",
    "#tensorboard = TensorBoard(log_dir=\"logs//{}\".format(NAME))\n",
    "\n",
    "# For finding and saving best model\n",
    "#filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "#checkpoint = ModelCheckpoint(\"models//{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed tensorboard callbacks, as it returns an error saying it can't create directories. Searched a number of forums, cant identify problem with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75002 samples, validate on 3812 samples\n",
      "Epoch 1/10\n",
      "75002/75002 [==============================] - 565s 8ms/sample - loss: 0.7131 - accuracy: 0.5202 - val_loss: 0.6886 - val_accuracy: 0.5367\n",
      "Epoch 2/10\n",
      "75002/75002 [==============================] - 551s 7ms/sample - loss: 0.6872 - accuracy: 0.5451 - val_loss: 0.6855 - val_accuracy: 0.5409\n",
      "Epoch 3/10\n",
      "75002/75002 [==============================] - 539s 7ms/sample - loss: 0.6829 - accuracy: 0.5586 - val_loss: 0.6777 - val_accuracy: 0.5632\n",
      "Epoch 4/10\n",
      "75002/75002 [==============================] - 541s 7ms/sample - loss: 0.6803 - accuracy: 0.5675 - val_loss: 0.6807 - val_accuracy: 0.5588\n",
      "Epoch 5/10\n",
      "75002/75002 [==============================] - 543s 7ms/sample - loss: 0.6785 - accuracy: 0.5702 - val_loss: 0.6796 - val_accuracy: 0.5669\n",
      "Epoch 6/10\n",
      "46912/75002 [=================>............] - ETA: 3:21 - loss: 0.6752 - accuracy: 0.5762"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, np.array(train_y),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, np.array(validation_y)),\n",
    "    #callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "if input(\"Save current model? (Y/N) \") == \"Y\":\n",
    "    model.save(\"models//{}\".format(NAME))\n",
    "    print(\"Save successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
